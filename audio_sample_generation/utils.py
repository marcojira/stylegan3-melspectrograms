import pickle
import librosa
import soundfile
import os
import numpy as np
from tqdm import tqdm
import torch

def to_audio(samples, output_path, sr=16000, n_fft=512, hop_length=256):
    """ Converts mel-spectrogram samples at samples_path to .wav samples stored in output_path

    :param samples: Mel-spectrograms of samples in tensor
    :param output_path: Where to output .wav samples
    :param sr: Sample rate to use for inversion
    :param n_fft: Length of window for inversion
    :param hop_length: Distance between successive windows
    """
    for i in tqdm(len(samples)):
        wav_path = os.path.join(output_path, f"{i}.wav")
        mel = samples[i]

        # Samples generated by StyleGAN2 are roughly between [-1, 1], transforms them to [0, 1]
        mel = (mel - mel.min())/(mel.max() - mel.min())
        # 6.907 and 4.859 are the max/min amplitudes used to get the image to be in [0, 1], this reverses the process
        mel = mel * (6.907  + 4.859)
        mel = mel - 6.907
        # Inverse of the transform to log scale
        mel = np.exp(2.0*mel)-1e-6 #

        # Inverse back to audio
        audio_data = librosa.feature.inverse.mel_to_audio(
            mel, sr=sr, n_fft=n_fft, hop_length=hop_length, norm=1
        )
        audio_data = audio_data.T
        soundfile.write(wav_path, audio_data, sr, format='WAV', subtype='FLOAT')


def create_sample_latent_pkl(G, num_samples, dimension, output_path, class_idx=None):
    """  Creates .pkl with generated mel-spectrograms and associated latents samples from network G

    :param G: Generator model
    :param num_samples: Number of samples to produce
    :param dimension: Output dimension of mel-spectrograms created by G (e.g. 128 for 128x128)
    :param output_path: Where to output the .pkl with samples
    :param class_idx: Index of class (None for unconditional generation)

    :return: Path to .pkl file where samples are stored
    """
    # Store samples
    samples = np.zeros((num_samples, 1, dimension, dimension))
    latents = np.zeros((num_samples, 1, dimension, dimension))
    num_batches = 10 if num_samples > 10 else 1 # Batch if more than 10 samples

    for i in range(num_batches):
        # Get boundaries of samples to produce for the current batch
        start, end = (num_samples//num_batches) * i, min((num_samples//num_batches) * (i+1), num_samples)

        z = torch.randn([end - start, G.z_dim]).cuda()

        # If the class index is set, use class conditional generation
        if class_idx:
            c = torch.zeros(end-start, 9).cuda()
            for j in range(end-start):
                c[j, class_idx] = 1
            samples[start: end] = G(z, c).cpu().numpy()
        else:
            samples[start:end] = G(z, class_idx).cpu().numpy()

        latents[start: end] = z

    # Store samples and latents
    samples_path = os.path.join(output_path, "samples.pkl")
    with open(samples_path, "wb") as f:
        pickle.dump(samples, f)

    latents_path = os.path.join(output_path, "latents.pkl")
    with open(latents_path, "wb") as f:
        pickle.dump(samples, f)

    return samples